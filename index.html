 <!DOCTYPE html>
<html lang="en">
<head>
  <title>MMWwand (IROS2024)</title>
  <meta name="description" content="Project page for Multi-Modal Representation Learning with Tactile Data.">
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=yes">
  <meta charset="utf-8">

  <!--Facebook preview-->
  <meta property="og:image" content="https://imagine.enpc.fr/~monniert/DTIClustering/thumbnail.png">
  <meta property="og:image:type" content="image/png">
  <meta property="og:image:width" content="600">
  <meta property="og:image:height" content="400">
  <meta property="og:type" content="website"/>
  <meta property="og:url" content="https://imagine.enpc.fr/~monniert/DTIClustering/"/>
  <meta property="og:title" content="DTI Clustering"/>
  <meta property="og:description" content="Project page for Deep Transformation-Invariant Clustering."/>

  <!--Twitter preview-->
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:title" content="DTI Clustering" />
  <meta name="twitter:description" content="Project page for Deep Transformation-Invariant Clustering."/>
  <meta name="twitter:image" content="https://imagine.enpc.fr/~monniert/DTIClustering/thumbnail_twitter.png">

  <!--Style-->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link href="style.css" rel="stylesheet">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

</head>
<body>

<div class="container" style="text-align:center; padding:2rem 15px">
  <div class="row" style="text-align:center">
    <h1>Multi-Modal Representation Learning with Tactile Data</h1>
    <h4>IROS 2024</h4>
  </div>
  <div class="row" style="text-align:center">
    <div class="col-xs-0 col-md-3"></div>
    <div class="col-xs-16 col-md-12">
      <h4>
        <a href="https://hyung-gun.me"><nobr>Hyung-gun Chi</nobr></a><sup>*,1</sup> &emsp;
        <span><nobr>Jose Barreiros</nobr></span><sup>*,2</sup> &emsp;
        <span><nobr>Jean Mercat</nobr></span><sup>2</sup>&emsp;
        <a href="https://engineering.purdue.edu/~ramani/wordpress/"><nobr>Karthik Ramani</nobr></a><sup>2</sup>&emsp;
        <span><nobr>Thomas Kollar</nobr></span><sup>2</sup>&emsp;
      </h4>
       <sup>*</sup> Equal contribution, <nobr><sup>1</sup>Purdue University</nobr>, <nobr><sup>2</sup>Toyota Research Institute</nobr>
    </div>
  </div>
</div>

<div class="container" style="text-align:center; padding:1rem">
  <img src="resrc/concept.png" alt="concept.jpg" class="text-center" style="width: 60%; max-width: 1100px">
  <h3 style="text-align:center; padding-top:1rem">
    <a class="label label-info" href="https://arxiv.org/abs/2006.11132">Paper</a>
    <a class="label label-info" href="">Video</a>
    <a class="label label-info" href="">Slides</a>
    <a class="label label-info" href="">Datasets</a>
  </h3>
</div>

<div class="container">
  <h3>Abstract</h3>
  <hr/>
  <p>
   Advancements in embodied language models like PALM-E and RT-2 have significantly enhanced
   language-conditioned robotic manipulation. However, these advances remain predominantly
   focused on vision and language, often overlooking the pivotal role of tactile feedback which
   is advantageous in contact-rich interactions. Our research introduces a novel approach that
   synergizes tactile information with vision and language. We present the Multi-Modal Wand
   (MMWand) dataset enriched with linguistic descriptions and tactile data. By integrating
   tactile feedback, we aim to bridge the divide between human linguistic understanding and
   robotic sensory interpretation. Our multi-modal representation model is trained on these
   datasets by employing the multi-modal embedding alignment principle from ImageBind which has
   shown promising results, emphasizing the potential of tactile data in robotic applications.
   The validation of our approach in downstream robotics tasks, such as texture-based object
   classification, cross-modality retrieval, and the dense reward function for visuomotor control,
   attests to its effectiveness. Our contributions underscore the importance of tactile feedback
   in multi-modal robotic learning and its potential to enhance robotic tasks.
  </p>

  <h3>MMWand Dataset</h3>
  <hr/>
  <div class="row" style="text-align:center; padding-left:1rem; padding-right:1rem; padding-bottom:1rem;">
    <h4><u>MMWand Design</u></h4>
      <p>For data collection, we installed three third-person view cameras. The MMWand is outfitted with tactile and force sensors,
      as well as an ego-view camera. actile information is gleaned using a GelSight Mini, a widely used visuo-tactile sensor that
      uses a camera looking at a deformable gel illuminated with structured light. A strain gauge load cell
      (5kg, 4-wire, Adafruit, ID: 4541) is integrated to record the total normal force exerted when the MMWand pushes against an object.
      Additionally, the wand includes a point-of-view RGB camera (Vitade webcam with an LED light ring, Full HD 1080P) located over
      the wand tip. The experimental bench is covered with a chroma-key green screen and includes three additional cameras that
      provide overhead and side views of the interaction.</p>
      <div class="col-xs-6">
        <img src="resrc/mmwand.png" alt="dti.png" class="text-center" style="width: 100%; max-width: 500px">
      </div>
      <div class="col-xs-6">
        <img src="resrc/mmwand_detail.jpg" alt="deep_tsf.png" class="text-center" style="width: 90%; max-width: 500px; margin-top:
        10px">
      </div>
  </div>
  <div class="row" style="text-align: center; padding-left:2rem; padding-right:1rem; padding-bottom:1rem;">
    <h4><u>Dataset Samples</u></h4>
      <p>The dataset comprises multi-view visual images, tactile images, and force values. Crucially, it offers language descriptions,
        which can facilitate the use of large language models for robotics tasks.</p>
      <img src="resrc/mmwand_sample.png" alt="" class="text-center" style="width: 100%; max-width: 1200px;">
      <div class="col-xs-6">
        <video controls style="width: 100%; max-width: 1200px;"><source src="resrc/demo1.mp4" type="video/mp4" /></video>
      </div>
      <div class="col-xs-6">
        <video controls style="width: 100%; max-width: 1200px;"><source src="resrc/demo2.mp4" type="video/mp4" /></video>
      </div>

  </div>
  <div class="row" style="text-align:center; padding-left:1rem; padding-right:1rem; padding-bottom:1rem;">
    <h4><u>Comparison with Other Tactile Datasets</u></h4>
      <p>MMWand is the only dataset offering a language description of objects while simultaneously encompassing high-quality touch
        acquisitions and images. In addition, it includes acquisition from different viewpoints and force measurements. These features
        are important assets for multi-modal representation learning and are needed to improve contact-rich applications in
        robotic manipulation.</p>
      <img src="resrc/comparison.png" alt="dti.png" class="text-center" style="width: 100%; max-width: 11000px">
  </div>

  <!-- <h3>Approach</h3> -->
  <!-- <hr/> -->
  <!-- <div class="row" style="text-align: center"> -->
  <!--   <div class="col-xs-6"> -->
  <!--     <h4 style="margin-right: 20%"><u>DTI framework</u></h4> -->
  <!--   </div> -->
  <!--   <div class="col-xs-6"> -->
  <!--     <h4><u>Deep transformation module -->
  <!--         <img src="http://latex.codecogs.com/svg.latex?\mathcal{T}_{f_{k}}" alt="T_f_k" border="0"/></u></h4> -->
  <!--   </div> -->
  <!-- </div> -->
  <!-- <div class="row" style="text-align: center"> -->
  <!--   <div class="col-xs-6"> -->
  <!--     <img src="resrc/dti.png" alt="dti.png" class="text-center" style="width: 100%; max-width: 900px"> -->
  <!--   </div> -->
  <!--   <div class="col-xs-6"> -->
  <!--     <img src="resrc/deep_tsf.png" alt="deep_tsf.png" class="text-center" style="width: 90%; max-width: 900px; margin-top: -->
  <!--     10px"> -->
  <!--   </div> -->
  <!-- </div> -->
  <!-- <div class="row" style="text-align: center"> -->
  <!--   <div class="col-xs-6"> -->
  <!--     <div style="width: 90%; max-width: 900px; padding-top:10px"> -->
  <!--     <p>Given a sample <img src="http://latex.codecogs.com/svg.latex?x_i" alt="x_i" border="0"/> and prototypes -->
  <!--     <img src="http://latex.codecogs.com/svg.latex?c_1" alt="c_1" border="0"/> and -->
  <!--     <img src="http://latex.codecogs.com/svg.latex?c_2" alt="c_2" border="0"/>, standard clustering such as K-means -->
  <!--     assigns the sample to the closest prototype. Our DTI clustering first aligns prototypes to the sample using a -->
  <!--     family of parametric transformations - here rotations - then picks the prototype whose alignment yields the -->
  <!--     smallest distance.</p> -->
  <!--     </div> -->
  <!--   </div> -->
  <!--   <div class="col-xs-6"> -->
  <!--     <div style="width: 100%; max-width: 900px; padding-top:10px"> -->
  <!--       <p>We predict alignment with deep learning. Given an image -->
  <!--     <img src="http://latex.codecogs.com/svg.latex?x_i" alt="x_i" border="0"/>, each deep parameter predictor -->
  <!--     <img src="http://latex.codecogs.com/svg.latex?f_k" alt="f_k" border="0"/> predicts -->
  <!--     parameters for a sequence of transformations - here affine, morphological and thin plate spline transformations - -->
  <!--     to align the prototype <img src="http://latex.codecogs.com/svg.latex?c_k" alt="c_k" border="0"/> -->
  <!--     to the query image <img src="http://latex.codecogs.com/svg.latex?x_i" alt="x_i" border="0"/>.</p> -->
  <!--     </div> -->
  <!--   </div> -->
  <!-- </div> -->


  <h3>Applications</h3>
  <hr/>
  <p>To highlight the benefits of our work in robotic applications, we measure the embedding distance from a current state to a goal state.
    To obtain embeddings, we trained ImageBind with different modalities (tactile, image, language)</p>
  <div class="row" style="text-align:center; padding-left:1rem; padding-right:1rem; padding-bottom:1rem;">
    <h4><u>Embedding Distance (Real-Robot Videos)</u></h4>
    <p>We execute a pick-and-place task using a Franka Research 3 robotic arm. We equipped the gripper with a 3D-printed mount that holds the tactile sensor.
    Additionally, the scene is recorded with an external camera. We define the last frame of the sequence as the target and compute the embedding distances
    of each frame with the target frame. The embedding distances were compared using only image, only tactile, and both image and tactile. The tactile input
    curve includes a step-like transition because the tactile image remains unchanged until the robot arm makes contact with an object. In contrast, the
    image input curve is smoother but displays noticeable “peaks”, which translates into a less informative cost function. By integrating both modalities,
    we achieve improved curves with fewer peaks and an earlier drop in the embedding distance. This feature enhances its applicability as a dense cost for
    visuomotor control since it encompasses the act of grasping and provides an earlier indication that the trajectory is correctly solving the task.</p>
      <div class="col-xs-6">
        <video controls style="width: 100%; max-width: 1200px;"><source src="resrc/demo5.mp4" type="video/mp4" /></video>
      </div>
      <div class="col-xs-6">
        <video controls style="width: 100%; max-width: 1200px;"><source src="resrc/demo6.mp4" type="video/mp4" /></video>
      </div>
      <div class="col-xs-6">
        <video controls style="width: 100%; max-width: 1200px;"><source src="resrc/demo7.mp4" type="video/mp4" /></video>
      </div>
      <div class="col-xs-6">
        <video controls style="width: 100%; max-width: 1200px;"><source src="resrc/demo8.mp4" type="video/mp4" /></video>
      </div>
  </div>
  <div class="row" style="text-align:center; padding-left:1rem; padding-right:1rem; padding-bottom:1rem;">
    <h4><u>Embedding Distance Comparison between Language and Tactile Image</u></h4>
      <p>We delve deeper by comparing the embedding distance plots, this time substituting the target tactile image with the object's texture described in
      language form. Owing to our model's training on aligning various modalities within the same embedding space, it is observed that even when the target
      tactile image is replaced with a textual texture description, the embedding distance continues to decrease following the tactile sensor''s contact
      with the object.</p>
      <div class="col-xs-6">
        <video controls style="width: 100%; max-width: 1200px;"><source src="resrc/demo3.mp4" type="video/mp4" /></video>
      </div>
      <div class="col-xs-6">
        <video controls style="width: 100%; max-width: 1200px;"><source src="resrc/demo4.mp4" type="video/mp4" /></video>
      </div>
      <div class="col-xs-6">
        <video controls style="width: 100%; max-width: 1200px;"><source src="resrc/demo9.mp4" type="video/mp4" /></video>
      </div>
      <div class="col-xs-6">
        <video controls style="width: 100%; max-width: 1200px;"><source src="resrc/demo10.mp4" type="video/mp4" /></video>
      </div>
  </div>

<h3>BibTex</h3>
  <p>If you find this work useful for your research, please cite:</p>
    <div class="card">
      <div class="card-block">
        <pre class="card-text clickselect">
@inproceedings{chi2024mmwand,
  title={{Multi-Modal Representation Learning with Tactile Data}},
  author={Chi, Hyung-gun and Barreiros, Jose and Mercat, Jean and Ramani, Karthik and Kollar, Thomas},
  booktitle={IROS},
  year={2024},
}</pre>
      </div>
    </div>

<div class="container" style="padding-top:3rem; padding-bottom:3rem">
  <p style="text-align:center">
  &#169; This webpage was in part inspired from this
  <a href="https://github.com/monniert/project-webpage">template</a>.
  </p>
</div>

</body>
</html>
